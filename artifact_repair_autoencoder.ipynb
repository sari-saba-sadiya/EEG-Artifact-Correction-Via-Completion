{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import json\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Dense, Activation, Permute, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D,UpSampling2D\n",
    "from keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the T1 samples before T2 and the T1 samples after T2+T3, T4 is\n",
    "# the specific sample we are goign to recreate\n",
    "T1 = 32\n",
    "T2 = 128\n",
    "T3 = 16\n",
    "T4 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_perc = 0.1 # Use 10 at each batch\n",
    "num_epochs = 1\n",
    "hp = np.load('../hyper_parameters.npy',allow_pickle=True)[()]\n",
    "h = 1 #int(sys.argv[1])\n",
    "avg_method = 1 #int(sys.argv[2])\n",
    "experiment = 'artifact_repair_'+str(h)+'_sample_'+str(T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.mkdir('Checkpoints/' + experiment)                       # make the directory '<time_val>'\n",
    "os.mkdir('Checkpoints/' + experiment + '/topology')         # make the directory 'topology'\n",
    "os.mkdir('Checkpoints/' + experiment + '/weights')          # make the directory for weights\n",
    "os.mkdir('Checkpoints/' + experiment + '/performance')      # make the directory for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks(experiment):\n",
    "    ###########################################################################\n",
    "    # Checkpoints \n",
    "    ###########################################################################\n",
    "    filepath=\"Checkpoints/\" + experiment + \"/weights/nn_weights-{epoch:02d}.hdf5\" # Where are checkpoints saved\n",
    "    STEPS_PER_EPOCH = 50\n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 filepath,\n",
    "                 monitor='val_loss',                     # Validation set Loss           \n",
    "                 verbose           = 0,                  # Display text \n",
    "                 save_weights_only = True,               # if True, only the model weights are saved\n",
    "                 save_best_only    = False,              # if True, the latest-best model is overwritten\n",
    "                 mode              = 'auto',             # used if 'save_best_only' is True\n",
    "                 steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                 save_freq         = 10)                 # Epochs between checkpoints\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Motor Imagery dataset\n",
    "class MotorImageryDataset:\n",
    "    def __init__(self, dataset='A01T.npz'):\n",
    "        if not dataset.endswith('.npz'):\n",
    "            dataset += '.npz'\n",
    "\n",
    "        self.data = np.load(dataset)\n",
    "\n",
    "        self.Fs = 250 # 250Hz from original paper\n",
    "\n",
    "        # keys of data ['s', 'etyp', 'epos', 'edur', 'artifacts']\n",
    "\n",
    "        self.raw = self.data['s'].T\n",
    "        self.events_type = self.data['etyp'].T\n",
    "        self.events_position = self.data['epos'].T\n",
    "        self.events_duration = self.data['edur'].T\n",
    "        self.artifacts = self.data['artifacts'].T\n",
    "\n",
    "        # Types of motor imagery\n",
    "        self.mi_types = {769: 'left', 770: 'right', 771: 'foot', 772: 'tongue', 783: 'unknown'}\n",
    "\n",
    "    def get_trials_from_channel(self):\n",
    "\n",
    "        # Channel default is C3\n",
    "\n",
    "        startrial_code = 768\n",
    "        starttrial_events = self.events_type == startrial_code\n",
    "        idxs = [i for i, x in enumerate(starttrial_events[0]) if x]\n",
    "\n",
    "        trials = []\n",
    "        classes = []\n",
    "        for index in idxs:\n",
    "            #try:\n",
    "            type_e = self.events_type[0, index+1]\n",
    "            if type_e not in self.mi_types.keys():\n",
    "                continue\n",
    "            class_e = self.mi_types[type_e]\n",
    "            if class_e == 'unknown':\n",
    "                continue\n",
    "            classes.append(type_e-769)\n",
    "\n",
    "            start = self.events_position[0, index] + 0.5 * self.Fs\n",
    "            stop = start + self.events_duration[0, index]\n",
    "            if stop < start + 2* self.Fs:\n",
    "                print(stop,start + 2* self.Fs)\n",
    "                raise '(VVO error): EEG is shorter than 2 sec'\n",
    "            #print(start,int(start + 2* self.Fs))\n",
    "            trial = signal.resample(self.raw[0:22, int(start):int(start + 2* self.Fs)],2*128,axis=1)\n",
    "            trials.append(trial.reshape(22,2*128,1))\n",
    "        return trials, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = []\n",
    "classes = []\n",
    "for file in glob.glob('../bcidatasetIV2a/*.npz'):\n",
    "    datasetA1 = MotorImageryDataset(file)\n",
    "    # trials contains the N valid trials, and clases its related class.\n",
    "    tmp_trials, tmp_classes = datasetA1.get_trials_from_channel()\n",
    "    trials.extend(tmp_trials)\n",
    "    classes.extend(tmp_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(trials, classes))\n",
    "random.shuffle(c)\n",
    "trials, classes = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = np.array([tr.squeeze() for tr in trials])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials shape: (2328, 22, 256)\n"
     ]
    }
   ],
   "source": [
    "print('Trials shape:',np.shape(trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(trials,\n",
    "                                                        classes, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ECC) Number of training trials=2095.\n",
      "(ECC) Number of validation trials=233.\n"
     ]
    }
   ],
   "source": [
    "print('(ECC) Number of training trials='+str(len(x_train))+'.\\n(ECC) Number of validation trials='+\n",
    "      str(len(x_val))+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,Chans,Samples = np.shape(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_main   = Input((1, Chans, Samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib.util\n",
    "#spec = importlib.util.spec_from_file_location(\"EEGExtract\", \"../EEGExtract/EEGExtract.py\")\n",
    "#foo = importlib.util.module_from_spec(spec)\n",
    "#spec.loader.exec_module(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input is T1 pointes before and after a missing T2 part\n",
    "Samples = T1*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "conv_sizes = []\n",
    "# The encoder\n",
    "for ii in range(hp['num_layers'][h]):\n",
    "    if ii == 0:\n",
    "        filter_shape = (22, 1)\n",
    "    else:\n",
    "        filter_shape = (11, 1)\n",
    "    nn.add(Conv2D(hp['kernal_sizes'][h][ii], filter_shape,\n",
    "                                     input_shape=(Chans, Samples,1),\n",
    "                                     kernel_constraint = max_norm(2., axis=(0,1,2)),\n",
    "                                     activation='tanh',\n",
    "                                     padding='same',\n",
    "                                     data_format       ='channels_last'))\n",
    "    conv_sizes.append(nn.layers[-1].output_shape[3])\n",
    "    if ii == 0:\n",
    "        nn.add(MaxPooling2D((2, 4), padding='same'))\n",
    "    else:\n",
    "        nn.add(MaxPooling2D((1, 4), padding='same'))\n",
    "        \n",
    "    if hp['dropouts'][h][ii] > 0:\n",
    "        nn.add(Dropout(hp['dropouts'][h][ii]))\n",
    "        \n",
    "    if hp['batch_norm'][h][ii] > 0 :\n",
    "        nn.add(BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1))\n",
    "        nn.add(BatchNormalization(axis=2, epsilon=1e-05, momentum=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder\n",
    "for jj in range(hp['num_layers'][h],0,-1):\n",
    "    ii = jj-1\n",
    "    if ii == 0:\n",
    "        filter_shape = (22, 1)\n",
    "    else:\n",
    "        filter_shape = (11, 1)\n",
    "    nn.add(Conv2D(conv_sizes[ii], filter_shape,\n",
    "                                     kernel_constraint = max_norm(2., axis=(0,1,2)),\n",
    "                                     activation='tanh',\n",
    "                                     padding='same',\n",
    "                                     data_format       ='channels_last'))\n",
    "\n",
    "    if ii == 0:\n",
    "        nn.add(UpSampling2D((2, 1)))\n",
    "    else:\n",
    "        nn.add(UpSampling2D((1, 1)))\n",
    "    # Add dropout\n",
    "    if hp['dropouts'][h][ii] > 0:\n",
    "        nn.add(Dropout(hp['dropouts'][h][ii]))\n",
    "    # Normalize?\n",
    "    if hp['batch_norm'][h][ii] > 0 :\n",
    "        nn.add(BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1))\n",
    "        nn.add(BatchNormalization(axis=2, epsilon=1e-05, momentum=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.add(Conv2D(1, (1, 1),\n",
    "             kernel_constraint = max_norm(2., axis=(0,1,2)),\n",
    "             activation='tanh',\n",
    "             padding='same',\n",
    "             data_format='channels_last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 22, 64, 64)        1472      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 11, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 16, 128)       90240     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 11, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 4, 256)        360704    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 11, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 1, 256)        721152    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 11, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 1, 128)        360576    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 11, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 1, 64)         180288    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 22, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 22, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 22, 1, 1)          65        \n",
      "=================================================================\n",
      "Total params: 1,714,497\n",
      "Trainable params: 1,714,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Should start with a 22,64,x and end with 22,1,1\n",
    "nn.build()\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates x = (22,2*T1) of the T1 samples before and after\n",
    "# y = (22,1) the T2+T4 sample from each original trial waveform \n",
    "def create_repair_data(x_train,N=10,T1=32,T2=128,T3=16,T4=1):\n",
    "    # Note that T3 must be smaller or equal than T2\n",
    "    #\n",
    "    if T4 > T3:\n",
    "        raise ValueError('Parameter T4 must be smaller or equal than T3')\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    N = 10\n",
    "    while len(x_out) <= N:\n",
    "        x = random.choice(x_train)\n",
    "        inputX  = np.hstack([x[:,T2-T1:T2],x[:,T2+T3:T2+T3+T1]]).reshape(x.shape[0],T1*2)\n",
    "        x_out.append(inputX)\n",
    "        y_out.append(x[:,T2+T4].reshape(x.shape[0],1))\n",
    "    return x_out,y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training vectors\n",
    "x_t,y_t = create_repair_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 64)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be 22 x 64\n",
    "x_t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation vectors\n",
    "x_v,y_v = create_repair_data(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(round(batch_perc*len(x_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_desc_algorithm = SGD(lr=hp['lr'][h], decay=0, momentum=hp['momentum'][h], nesterov=bool(hp['nesterov'][h]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = callbacks(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(loss='mean_squared_error', optimizer=grad_desc_algorithm, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(np.array(x_t),\n",
    "         np.array(y_t),\n",
    "         validation_data = (np.array(x_v),np.array(y_v)),\n",
    "         epochs = num_epochs,\n",
    "         initial_epoch = 0,\n",
    "         batch_size = batch_size,\n",
    "         verbose =1,\n",
    "         callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
