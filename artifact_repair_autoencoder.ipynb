{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import json\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Dense, Activation, Permute, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D,UpSampling2D\n",
    "from keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.backend import tensorflow_backend\n",
    "import tensorflow\n",
    "#dir(tensorflow.keras.backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_perc = 0.1 # Use 10 at each batch\n",
    "num_epochs = 1\n",
    "hp = np.load('hyper_parameters.npy',allow_pickle=True)[()]\n",
    "h = 1 #int(sys.argv[1])\n",
    "avg_method = 1 #int(sys.argv[2])\n",
    "experiment = 'ECC_model'+str(h)+'_avg'+str(avg_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'Checkpoints/ECC_model1_avg1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-34798ff3766d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoints/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m# make the directory '<time_val>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoints/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/topology'\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# make the directory 'topology'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoints/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/weights'\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# make the directory for weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoints/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/performance'\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# make the directory for performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'Checkpoints/ECC_model1_avg1'"
     ]
    }
   ],
   "source": [
    "os.mkdir('Checkpoints/' + experiment)                       # make the directory '<time_val>'\n",
    "os.mkdir('Checkpoints/' + experiment + '/topology')         # make the directory 'topology'\n",
    "os.mkdir('Checkpoints/' + experiment + '/weights')          # make the directory for weights\n",
    "os.mkdir('Checkpoints/' + experiment + '/performance')      # make the directory for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return K.square(x)\n",
    "def log(x):\n",
    "    return K.log(K.clip(x, min_value = 1e-7, max_value = 10000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks(experiment):\n",
    "    ###########################################################################\n",
    "    # Checkpoints \n",
    "    ###########################################################################\n",
    "    filepath=\"Checkpoints/\" + experiment + \"/weights/nn_weights-{epoch:02d}.hdf5\" # Where are checkpoints saved\n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 filepath,\n",
    "                 monitor='val_loss',                     # Validation set Loss           \n",
    "                 verbose           = 0,                  # Display text \n",
    "                 save_weights_only = True,               # if True, only the model weights are saved\n",
    "                 save_best_only    = False,              # if True, the latest-best model is overwritten\n",
    "                 mode              = 'auto',             # used if 'save_best_only' is True  \n",
    "                 save_freq         = 10)                 # Epochs between checkpoints\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotorImageryDataset:\n",
    "    def __init__(self, dataset='A01T.npz'):\n",
    "        if not dataset.endswith('.npz'):\n",
    "            dataset += '.npz'\n",
    "\n",
    "        self.data = np.load(dataset)\n",
    "\n",
    "        self.Fs = 250 # 250Hz from original paper\n",
    "\n",
    "        # keys of data ['s', 'etyp', 'epos', 'edur', 'artifacts']\n",
    "\n",
    "        self.raw = self.data['s'].T\n",
    "        self.events_type = self.data['etyp'].T\n",
    "        self.events_position = self.data['epos'].T\n",
    "        self.events_duration = self.data['edur'].T\n",
    "        self.artifacts = self.data['artifacts'].T\n",
    "\n",
    "        # Types of motor imagery\n",
    "        self.mi_types = {769: 'left', 770: 'right', 771: 'foot', 772: 'tongue', 783: 'unknown'}\n",
    "\n",
    "    def get_trials_from_channel(self):\n",
    "\n",
    "        # Channel default is C3\n",
    "\n",
    "        startrial_code = 768\n",
    "        starttrial_events = self.events_type == startrial_code\n",
    "        idxs = [i for i, x in enumerate(starttrial_events[0]) if x]\n",
    "\n",
    "        trials = []\n",
    "        classes = []\n",
    "        for index in idxs:\n",
    "            #try:\n",
    "            type_e = self.events_type[0, index+1]\n",
    "            if type_e not in self.mi_types.keys():\n",
    "                continue\n",
    "            class_e = self.mi_types[type_e]\n",
    "            if class_e == 'unknown':\n",
    "                continue\n",
    "            classes.append(type_e-769)\n",
    "\n",
    "            start = self.events_position[0, index] + 0.5 * self.Fs\n",
    "            stop = start + self.events_duration[0, index]\n",
    "            if stop < start + 2* self.Fs:\n",
    "                print(stop,start + 2* self.Fs)\n",
    "                raise '(VVO error): EEG is shorter than 2 sec'\n",
    "            #print(start,int(start + 2* self.Fs))\n",
    "            trial = signal.resample(self.raw[0:22, int(start):int(start + 2* self.Fs)],2*128,axis=1)\n",
    "            trials.append(trial.reshape(22,2*128,1))\n",
    "\n",
    "            #except:\n",
    "            #    continue\n",
    "\n",
    "        return trials, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_trials(x_list,y_list,average=0,k=10):\n",
    "    x_dict = {}\n",
    "    for x,y in zip(x_list,y_list):\n",
    "        if y not in x_dict.keys():\n",
    "            x_dict[y] = []\n",
    "        x_dict[y].append(x)\n",
    "    out_list_x = []\n",
    "    out_list_y = []\n",
    "    out_list_l = []\n",
    "    for y in x_dict.keys():\n",
    "        #print(y,len(out_list_x))\n",
    "        tmp_list_x = x_dict[y]\n",
    "        tmp_list_y = []\n",
    "        if average == 0:\n",
    "            tmp_list_y = x_dict[y]\n",
    "            random.shuffle(tmp_list_y)\n",
    "        else:\n",
    "            for ii,x in enumerate(tmp_list_x):\n",
    "                tmp_list_y.append(np.mean(random.choices(tmp_list_x, k=k), axis=0))\n",
    "        out_list_x.extend(tmp_list_x)\n",
    "        out_list_y.extend(tmp_list_y)\n",
    "        out_list_l.extend([y]*len(tmp_list_y))\n",
    "    return out_list_x,out_list_y,out_list_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = []\n",
    "classes = []\n",
    "for file in glob.glob('./bcidatasetIV2a/*.npz'):\n",
    "    datasetA1 = MotorImageryDataset(file)\n",
    "    tmp_trials, tmp_classes = datasetA1.get_trials_from_channel() # trials contains the N valid trials, and clases its related class.\n",
    "    trials.extend(tmp_trials)\n",
    "    classes.extend(tmp_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(trials, classes))\n",
    "random.shuffle(c)\n",
    "trials, classes = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials shape: (2328, 22, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Trials shape:',np.shape(trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(trials, classes, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ECC) Number of training trials=2095.\n",
      "(ECC) Number of validation trials=233.\n"
     ]
    }
   ],
   "source": [
    "print('(ECC) Number of training trials='+str(len(x_train))+'.\\n(ECC) Number of validation trials='+str(len(x_val))+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,Chans,Samples,b = np.shape(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_main   = Input((1, Chans, Samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Samples = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "conv_sizes = []\n",
    "# The encoder\n",
    "for ii in range(hp['num_layers'][h]):\n",
    "    if ii == 0:\n",
    "        filter_shape = (22, 1)\n",
    "    else:\n",
    "        filter_shape = (11, 1)\n",
    "    nn.add(Conv2D(hp['kernal_sizes'][h][ii], filter_shape,\n",
    "                                     input_shape=(Chans, Samples,1),\n",
    "                                     kernel_constraint = max_norm(2., axis=(0,1,2)),\n",
    "                                     activation='tanh',\n",
    "                                     padding='same',\n",
    "                                     data_format       ='channels_last'))\n",
    "    conv_sizes.append(nn.layers[-1].output_shape[3])\n",
    "    if ii == 0:\n",
    "        nn.add(MaxPooling2D((2, 4), padding='same'))\n",
    "    else:\n",
    "        nn.add(MaxPooling2D((1, 4), padding='same'))\n",
    "        \n",
    "    if hp['dropouts'][h][ii] > 0:\n",
    "        nn.add(Dropout(hp['dropouts'][h][ii]))\n",
    "        \n",
    "    if hp['batch_norm'][h][ii] > 0 :\n",
    "        nn.add(BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1))\n",
    "        nn.add(BatchNormalization(axis=2, epsilon=1e-05, momentum=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder\n",
    "for jj in range(hp['num_layers'][h],0,-1):\n",
    "    ii = jj-1\n",
    "    if ii == 0:\n",
    "        filter_shape = (22, 1)\n",
    "    else:\n",
    "        filter_shape = (11, 1)\n",
    "    nn.add(Conv2D(conv_sizes[ii], filter_shape,\n",
    "                                     kernel_constraint = max_norm(2., axis=(0,1,2)),\n",
    "                                     activation='tanh',\n",
    "                                     padding='same',\n",
    "                                     data_format       ='channels_last'))\n",
    "\n",
    "    if ii == 0:\n",
    "        nn.add(UpSampling2D((2, 1)))\n",
    "    else:\n",
    "        nn.add(UpSampling2D((1, 1)))\n",
    "        \n",
    "        \n",
    "    if hp['dropouts'][h][ii] > 0:\n",
    "        nn.add(Dropout(hp['dropouts'][h][ii]))\n",
    "        \n",
    "    if hp['batch_norm'][h][ii] > 0 :\n",
    "        nn.add(BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1))\n",
    "        nn.add(BatchNormalization(axis=2, epsilon=1e-05, momentum=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.add(Conv2D(1, (1, 1),\n",
    "             kernel_constraint = max_norm(2., axis=(0,1,2)),\n",
    "             activation='tanh',\n",
    "             padding='same',\n",
    "             data_format='channels_last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 22, 64, 64)        1472      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 11, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 16, 128)       90240     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 11, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 4, 256)        360704    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 11, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 1, 256)        721152    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 11, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 1, 128)        360576    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 11, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 1, 64)         180288    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 22, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 22, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 22, 1, 1)          65        \n",
      "=================================================================\n",
      "Total params: 1,714,497\n",
      "Trainable params: 1,714,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.build()\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repair_data(x_train,N=10,T1=32,T2=3):\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    N = 10\n",
    "    T1 = 2\n",
    "    T2 = 3\n",
    "    while len(x_out) <= N:\n",
    "        x = random.choice(x_train)\n",
    "        beginIdx = np.random.randint(x.shape[1]-(2*T1+T2))\n",
    "        endIdx = beginIdx+2*T1+T2\n",
    "        yIdx = beginIdx+T1+T2\n",
    "        inputX  = np.hstack([x[:,beginIdx:beginIdx+T1,:],x[:,endIdx-T1:endIdx,:]]).reshape(x.shape[0],2,1)\n",
    "        x_out.append(inputX)\n",
    "        y_out.append(x[:,yIdx,:].reshape(x.shape[0],1,1))\n",
    "    return x_out,y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t,y_t = create_repair_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 2, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_v,y_v = create_repair_data(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(round(batch_perc*len(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_desc_algorithm = SGD(lr=hp['lr'][h], decay=0, momentum=hp['momentum'][h], nesterov=bool(hp['nesterov'][h]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = callbacks(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(loss='mean_squared_error', optimizer=grad_desc_algorithm, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 11 samples\n",
      "11/11 [==============================] - 3s 257ms/sample - loss: 143.7677 - accuracy: 0.0000e+00 - val_loss: 87.6242 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4334c70e10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(np.array(x_t),\n",
    "         np.array(y_t),\n",
    "         validation_data = (np.array(x_v),np.array(y_v)),\n",
    "         epochs = num_epochs,\n",
    "         initial_epoch = 0,\n",
    "         batch_size = batch_size,\n",
    "         verbose =1,\n",
    "         callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
